{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Tinmar:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Transformations to Populate our Data Warehouse</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x29d9f7b76d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import pandas as pd\n",
    "import ast\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable \n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = '1' \n",
    "\n",
    "# imports para trabajar con spark en local\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import IntegerType,StructField,StructType,StringType,LongType,MapType\n",
    "from pyspark.sql import functions as F\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "# imports para trabajar con GCP\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import translate_v2 as translate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Credenciales para API's de GCP\n",
    "SERVICE_ACCOUNT_FILE = '../../credentials/fiery-protocol-399500-f2566dd92ef4.json'\n",
    "creds = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE)\n",
    "translate_client = translate.Client(credentials=creds)\n",
    "\n",
    "\n",
    "# Iniciamos sesi√≥n de spark.\n",
    "SparkSession.stop(spk)\n",
    "spk = SparkSession.builder.appName(\"PySpark Transformations to Populate our Data Warehouse\").master(\"local[1]\").getOrCreate()\n",
    "ps.options.compute.ops_on_diff_frames = True\n",
    "spk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(gmap_id    object\n",
       " user_id    object\n",
       " name       object\n",
       " time        int64\n",
       " rating      int64\n",
       " text       object\n",
       " resp       object\n",
       " dtype: object,\n",
       " gmap_id    150000\n",
       " user_id    150000\n",
       " name       150000\n",
       " time       150000\n",
       " rating     150000\n",
       " text        88255\n",
       " resp        22406\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf = ps.read_json('../../data/Google Maps/estados/review-Texas/11.json',index_col='gmap_id').reset_index()[['gmap_id', 'user_id', 'name', 'time', 'rating', 'text', 'resp']].spark.cache()\n",
    "psdf.dtypes, psdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gmap_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>resp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x864c23ee6b5d9d09:0x3cc9cba7f179b2ee</td>\n",
       "      <td>102540505680898147322</td>\n",
       "      <td>Sergio Orjuela</td>\n",
       "      <td>1569461702556</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>(We greatly appreciate you stopping by our sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x864c23ee6b5d9d09:0x3cc9cba7f179b2ee</td>\n",
       "      <td>114879155270428006890</td>\n",
       "      <td>olivia</td>\n",
       "      <td>1620775162751</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>(Thank you for your support ü•∞, 1622864960517)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 gmap_id                user_id            name           time  rating  text                                                                                                                            resp\n",
       "0  0x864c23ee6b5d9d09:0x3cc9cba7f179b2ee  102540505680898147322  Sergio Orjuela  1569461702556       5  None  (We greatly appreciate you stopping by our store and enjoy our premium drinks\\n- Tocotoco Tea Management Team-, 1569937557198)\n",
       "1  0x864c23ee6b5d9d09:0x3cc9cba7f179b2ee  114879155270428006890          olivia  1620775162751       5  None                                                                                   (Thank you for your support ü•∞, 1622864960517)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark.pandas data frame persisted\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField('user_id',StringType(),False),\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('time',LongType(),True),\n",
    "    StructField('rating',IntegerType(),True),\n",
    "    StructField('text',StringType(),True),\n",
    "    StructField('resp',MapType(StringType(),StringType()),False),\n",
    "    StructField('gmap_id',StringType(),False)\n",
    "])\n",
    "\n",
    "\n",
    "i = 1\n",
    "df_list = []\n",
    "while True:\n",
    "    try:\n",
    "        # Leemos los archivos en un SPARK Data Frame para poder acceder directamente a GCS\n",
    "        sdf = spk.read.schema(schema).json(f'../../data/Google Maps/estados/review-California/{i}.json')[['gmap_id', 'user_id', 'name', 'time', 'rating', 'text', 'resp']].cache()\n",
    "        sdf.selectExpr('cast(user_id as int) user_id').cache()\n",
    "        sdf = sdf.withColumn('tiempo_respuesta',F.col('resp').getField('time')).cache()\n",
    "        sdf = sdf.withColumn('tiempo_respuesta',F.col('tiempo_respuesta').cast(LongType())).cache()\n",
    "        sdf = sdf.withColumn('respuesta',F.col('resp').getField('text')).sort('user_id').cache()\n",
    "        sdf = sdf.withColumn('time',F.col('time').cast(LongType())).cache()\n",
    "        # sdf = sdf[['index','gmap_id','user_id','name','time','rating','text','tiempo_respuesta','respuesta']].cache()\n",
    "        # PANDAS API Data Frame: Paso intermedio para trabajar con los m√©todos de pandas pero con la potencia de spark, posteriormente guardaremos los datos en BQ despu√©s de \n",
    "        # las transformaciones...\n",
    "        # sdf.count()\n",
    "        psdf = sdf.pandas_api()\n",
    "        sdf.unpersist()\n",
    "        del sdf\n",
    "        gc.collect()\n",
    "        # psdf['estado'] = state\n",
    "        df_list.append(psdf)\n",
    "        i += 1\n",
    "    except AnalysisException:\n",
    "        break\n",
    "\n",
    "psdf = ps.concat(df_list,axis=0)\n",
    "del df_list\n",
    "gc.collect()\n",
    "psdf = psdf.reset_index()\n",
    "psdf.spark.cache()\n",
    "print(f'pyspark.pandas data frame persisted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+-------------+------+--------------------+----------------+--------------------+\n",
      "| index|             gmap_id|             user_id|                name|         time|rating|                text|tiempo_respuesta|           respuesta|\n",
      "+------+--------------------+--------------------+--------------------+-------------+------+--------------------+----------------+--------------------+\n",
      "| 71227|0x80c2c5b46727768...|10000000910516696...|          Nidia Arce|1534331964232|     5|        Good serves!|            NULL|                NULL|\n",
      "|106783|0x80dca0ab76284ae...|10000001110383985...|Johnathan Kirkcon...|1592880304004|     4|                NULL|            NULL|                NULL|\n",
      "| 43781|0x808580e31c85111...|10000002480984360...|       Victor Medina|1493337633947|     5|Very nice establi...|   1493759986241|Thanks for the 5/...|\n",
      "| 85594|0x80eacb93b18677b...|10000002997950820...|       Raychel Perez|1593401857411|     5|                NULL|            NULL|                NULL|\n",
      "| 66345|0x80952621411312e...|10000002997950820...|       Raychel Perez|1557096517534|     4|                NULL|            NULL|                NULL|\n",
      "| 17889|0x80eaa9d52530298...|10000002997950820...|       Raychel Perez|1561259731980|     4|                NULL|            NULL|                NULL|\n",
      "| 34260|0x80952f37d017068...|10000002997950820...|       Raychel Perez|1557095912320|     4|                NULL|            NULL|                NULL|\n",
      "| 42420|0x80dce08ae58b4bc...|10000004165687973...|       Brittany Webb|1570887408667|     3|                NULL|            NULL|                NULL|\n",
      "| 79553|0x80dce06daad4bda...|10000004165687973...|       Brittany Webb|1570887452018|     5|                NULL|            NULL|                NULL|\n",
      "|108831|0x80dce0704eb5c52...|10000004165687973...|       Brittany Webb|1525446286769|     4|Parking can be tr...|            NULL|                NULL|\n",
      "|114705|0x80c2a4c86efe3f7...|10000004165687973...|       Brittany Webb|1570887129917|     4|                NULL|            NULL|                NULL|\n",
      "|103649|0x80dca0ab76284ae...|10000004365320516...| Christina Babiegirl|1563138801400|     5|This is our favor...|            NULL|                NULL|\n",
      "| 90476|0x80dd29e9914f603...|10000004859928366...|    liat/eran salmon|1540896195684|     4|(Translated by Go...|            NULL|                NULL|\n",
      "|123440|0x808fca855c87980...|10000006169280173...|        Daniel Field|1606844805502|     5|Don‚Äôt let the bra...|   1611589317571|Hello Daniel, it ...|\n",
      "| 45372|0x80c333273fd97fb...|10000010402273743...|                Âç¢Â®ü|1569043508145|     5|                NULL|            NULL|                NULL|\n",
      "|109238|0x80c331f3c4d53ca...|10000010402273743...|                Âç¢Â®ü|1569532884345|     5|                NULL|            NULL|                NULL|\n",
      "|131585|0x8085625a787bed7...|10000010657618606...|          Anne Mason|1488048265269|     4|        Food is good|            NULL|                NULL|\n",
      "|144908|0x8085806514f434a...|10000010657618606...|          Anne Mason|1495990925999|     5|Great price on ev...|            NULL|                NULL|\n",
      "| 24517|0x808559bb7048466...|10000010657618606...|          Anne Mason|1481038674775|     3|                NULL|            NULL|                NULL|\n",
      "| 87771|0x808f7e1b4ff8617...|10000010657618606...|          Anne Mason|1495990793995|     5|Vintage, easy to ...|            NULL|                NULL|\n",
      "+------+--------------------+--------------------+--------------------+-------------+------+--------------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sdf.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = false)\n",
      " |-- gmap_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- tiempo_respuesta: long (nullable = true)\n",
      " |-- respuesta: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2700000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index      2700000\n",
       "gmap_id    2700000\n",
       "user_id    2700000\n",
       "name       2700000\n",
       "time       2700000\n",
       "rating     2700000\n",
       "text       1529036\n",
       "resp        245169\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>af</td>\n",
       "      <td>Afrikaans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sq</td>\n",
       "      <td>Albanian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language       name\n",
       "0       af  Afrikaans\n",
       "1       sq   Albanian"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages_psdf = ps.DataFrame(translate_client.get_languages()).sort_values('name')\n",
    "languages_psdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"time\":1629166810138,\"text\":\"Áé∞Âú®Êé•Á∫øÂëòÊòØÂæàÂ•ΩÁöÑËã±ËØ≠\"}'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf.resp.loc[66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Áé∞Âú®Êé•Á∫øÂëòÊòØÂæàÂ•ΩÁöÑËã±ËØ≠'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf.resp.loc[66][30:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf['text'] = psdf.text.fillna('nan')\n",
    "sdf = sdf.withColumn(\"resp\", F.to_json(\"resp\"))\n",
    "respuestas = sdf.pandas_api()\n",
    "respuestas = respuestas[['resp']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\pyspark\\pandas\\frame.py:13566\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m  13565\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m> 13566\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc[:, key]\n\u001b[0;32m  13567\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\pyspark\\pandas\\indexing.py:487\u001b[0m, in \u001b[0;36mLocIndexerLike.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    480\u001b[0m cond, limit, remaining_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_rows(rows_sel)\n\u001b[0;32m    481\u001b[0m (\n\u001b[0;32m    482\u001b[0m     column_labels,\n\u001b[0;32m    483\u001b[0m     data_spark_columns,\n\u001b[0;32m    484\u001b[0m     data_fields,\n\u001b[0;32m    485\u001b[0m     returns_series,\n\u001b[0;32m    486\u001b[0m     series_name,\n\u001b[1;32m--> 487\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_select_cols(cols_sel)\n\u001b[0;32m    489\u001b[0m \u001b[39mif\u001b[39;00m cond \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m limit \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m returns_series:\n",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\pyspark\\pandas\\indexing.py:334\u001b[0m, in \u001b[0;36mLocIndexerLike._select_cols\u001b[1;34m(self, cols_sel, missing_keys)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_select_cols_else(cols_sel, missing_keys)\n",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\pyspark\\pandas\\indexing.py:1392\u001b[0m, in \u001b[0;36mLocIndexer._select_cols_else\u001b[1;34m(self, cols_sel, missing_keys)\u001b[0m\n\u001b[0;32m   1391\u001b[0m     cols_sel \u001b[39m=\u001b[39m (cols_sel,)\n\u001b[1;32m-> 1392\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_from_multiindex_column(cols_sel, missing_keys)\n",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\pyspark\\pandas\\indexing.py:1210\u001b[0m, in \u001b[0;36mLocIndexer._get_from_multiindex_column\u001b[1;34m(self, key, missing_keys, labels, recursed)\u001b[0m\n\u001b[0;32m   1209\u001b[0m \u001b[39mif\u001b[39;00m missing_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1210\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(k)\n\u001b[0;32m   1211\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tinma\\OneDrive\\Escritorio\\HENRY\\Proyecto_Grupal_HENRY\\spark\\local_testing\\pyspark_testing.ipynb Celda 13\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tinma/OneDrive/Escritorio/HENRY/Proyecto_Grupal_HENRY/spark/local_testing/pyspark_testing.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m respuestas\u001b[39m.\u001b[39mspark\u001b[39m.\u001b[39mcache()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tinma/OneDrive/Escritorio/HENRY/Proyecto_Grupal_HENRY/spark/local_testing/pyspark_testing.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# respuestas = respuestas.astype({'resp':str}).spark.cache()\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tinma/OneDrive/Escritorio/HENRY/Proyecto_Grupal_HENRY/spark/local_testing/pyspark_testing.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m respuestas[\u001b[39m'\u001b[39m\u001b[39mresp\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m respuestas\u001b[39m.\u001b[39;49mtext\u001b[39m.\u001b[39mfillna(\u001b[39m'\u001b[39m\u001b[39mnan\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tinma/OneDrive/Escritorio/HENRY/Proyecto_Grupal_HENRY/spark/local_testing/pyspark_testing.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m respuestas \u001b[39m=\u001b[39m respuestas\u001b[39m.\u001b[39mresp \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnan\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tinma/OneDrive/Escritorio/HENRY/Proyecto_Grupal_HENRY/spark/local_testing/pyspark_testing.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m resp_idx \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(respuestas\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mtolist())\n",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\pyspark\\pandas\\frame.py:13568\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m  13566\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc[:, key]\n\u001b[0;32m  13567\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m> 13568\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m  13569\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, key)\n\u001b[0;32m  13570\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "psdf['text'] = psdf.text.fillna('nan')\n",
    "sdf = sdf.withColumn(\"resp\", F.to_json(\"resp\"))\n",
    "respuestas = sdf.pandas_api()\n",
    "respuestas = respuestas[['resp']]\n",
    "respuestas.spark.cache()\n",
    "respuestas['resp'] = respuestas.text.fillna('nan')\n",
    "respuestas = respuestas.resp != 'nan'\n",
    "resp_idx = sorted(respuestas.index.tolist())\n",
    "print(respuestas.head(2))\n",
    "resp_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"time\":1629166810138,\"text\":\"The operator now speaks very good English.\"}'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf.resp.loc[66][:30] + f'{translation[\"translatedText\"]}.\"' + '}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': 1629166810138, 'text': 'The operator now speaks very good English.'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(psdf.resp.loc[66][:30] + f'{translation[\"translatedText\"]}.\"' + '}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"time\":1631072651706,\"text\":\"Thanks so much for your business and for taking the time to post a great review. Glad to hear our crew was so helpful and friendly and that we were able to get the family outfitted quickly and efficiently. Look forward to hearing from you in the future.\"}'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuestas.loc[30,'resp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "ename": "SparkRuntimeException",
     "evalue": "[UNSUPPORTED_FEATURE.LITERAL_TYPE] The feature is not supported: Literal for '{time=1631072651706, text=Thanks so much for your business and for taking the time to post a great review. Glad to hear our crew was so helpful and friendly and that we were able to get the family outfitted quickly and efficiently. Look forward to hearing from you in the future.}' of class java.util.HashMap.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSparkRuntimeException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tinma\\OneDrive\\Escritorio\\HENRY\\Proyecto_Grupal_HENRY\\spark\\local_testing\\pyspark_testing.ipynb Celda 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tinma/OneDrive/Escritorio/HENRY/Proyecto_Grupal_HENRY/spark/local_testing/pyspark_testing.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tinma/OneDrive/Escritorio/HENRY/Proyecto_Grupal_HENRY/spark/local_testing/pyspark_testing.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     psdf\u001b[39m.\u001b[39mloc[i,\u001b[39m'\u001b[39m\u001b[39midioma_respuesta\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39men\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tinma/OneDrive/Escritorio/HENRY/Proyecto_Grupal_HENRY/spark/local_testing/pyspark_testing.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     psdf\u001b[39m.\u001b[39;49mloc[i,\u001b[39m'\u001b[39;49m\u001b[39mresp\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m=\u001b[39m ast\u001b[39m.\u001b[39mliteral_eval(respuestas\u001b[39m.\u001b[39mloc[i,\u001b[39m'\u001b[39m\u001b[39mresp\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tinma/OneDrive/Escritorio/HENRY/Proyecto_Grupal_HENRY/spark/local_testing/pyspark_testing.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m \u001b[39m100\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tinma/OneDrive/Escritorio/HENRY/Proyecto_Grupal_HENRY/spark/local_testing/pyspark_testing.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\pyspark\\pandas\\indexing.py:737\u001b[0m, in \u001b[0;36mLocIndexerLike.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    735\u001b[0m         value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mspark\u001b[39m.\u001b[39mcolumn\n\u001b[0;32m    736\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 737\u001b[0m     value \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mlit(value)\n\u001b[0;32m    739\u001b[0m new_data_spark_columns \u001b[39m=\u001b[39m []\n\u001b[0;32m    740\u001b[0m new_fields \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\pyspark\\sql\\utils.py:174\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(functions, f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    173\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\pyspark\\sql\\functions.py:193\u001b[0m, in \u001b[0;36mlit\u001b[1;34m(col)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[39mif\u001b[39;00m dt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m         \u001b[39mreturn\u001b[39;00m _invoke_function(\u001b[39m\"\u001b[39m\u001b[39mlit\u001b[39m\u001b[39m\"\u001b[39m, col)\u001b[39m.\u001b[39mastype(dt)\u001b[39m.\u001b[39malias(\u001b[39mstr\u001b[39m(col))\n\u001b[1;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m _invoke_function(\u001b[39m\"\u001b[39;49m\u001b[39mlit\u001b[39;49m\u001b[39m\"\u001b[39;49m, col)\n",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\pyspark\\sql\\functions.py:97\u001b[0m, in \u001b[0;36m_invoke_function\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     96\u001b[0m jf \u001b[39m=\u001b[39m _get_jvm_function(name, SparkContext\u001b[39m.\u001b[39m_active_spark_context)\n\u001b[1;32m---> 97\u001b[0m \u001b[39mreturn\u001b[39;00m Column(jf(\u001b[39m*\u001b[39;49margs))\n",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mSparkRuntimeException\u001b[0m: [UNSUPPORTED_FEATURE.LITERAL_TYPE] The feature is not supported: Literal for '{time=1631072651706, text=Thanks so much for your business and for taking the time to post a great review. Glad to hear our crew was so helpful and friendly and that we were able to get the family outfitted quickly and efficiently. Look forward to hearing from you in the future.}' of class java.util.HashMap."
     ]
    }
   ],
   "source": [
    "psdf['idioma_respuesta'] = ps.Series([],dtype='str')\n",
    "respuestas.spark.cache()\n",
    "for i in resp_idx:\n",
    "    print(i)\n",
    "    respuesta = respuestas.loc[i,'resp'][30:-2]\n",
    "    language = translate_client.detect_language(respuesta)\n",
    "    if language['language'] != 'en':\n",
    "        translation = translate_client.translate(respuesta, target_language='en')\n",
    "        psdf.loc[i,'idioma_respuesta'] = language['language']\n",
    "        psdf.loc[i,'resp'] = ast.literal_eval(respuestas.loc[i,'resp'][:30] + f'{translation[\"translatedText\"]}.\"' + '}')\n",
    "    else:\n",
    "        psdf.loc[i,'idioma_respuesta'] = 'en'\n",
    "        psdf.loc[i,'resp'] = ast.literal_eval(respuestas.loc[i,'resp'])\n",
    "    if i > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(respuestas)):\n",
    "    respuestas['resp'].apply(lambda x: ast.literal_eval(x))\n",
    "    respuestas.loc[i,'resp_time'] = respuestas.loc[i,'resp']['time']\n",
    "    respuestas.loc[i,'resp_text'] = respuestas.loc[i,'resp']['text']\n",
    "respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'fiery-protocol-399500'\n",
    "STATES = ['California'] #,'Texas'] # ,'New_York','Colorado','Georgia']\n",
    "schema = StructType([\n",
    "    StructField('user_id',StringType(),False),\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('time',LongType(),True),\n",
    "    StructField('rating',IntegerType(),True),\n",
    "    StructField('text',StringType(),True),\n",
    "    StructField('resp',StringType(),False),\n",
    "    StructField('gmap_id',StringType(),False)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "psdfx = ps.DataFrame(columns=['gmap_id','user_id','name','time','text','rating','resp_time','resp_text'])\n",
    "for state in STATES:\n",
    "    i = 1\n",
    "    df_list = []\n",
    "    while True:\n",
    "        try:\n",
    "            # Leemos los archivos en un SPARK Data Frame para poder acceder directamente a GCS\n",
    "            sdf = spk.read.schema(schema).json(f'../data/Google Maps/estados/review-{state}/{i}.json')[['user_id','name','time','rating','text','resp','gmap_id']].cache()\n",
    "            sdf.selectExpr('cast(user_id as int) user_id')\n",
    "            sdf.selectExpr('cast(null as map<string,string>) resp')\n",
    "            # PANDAS API Data Frame: Paso intermedio para trabajar con los m√©todos de pandas pero con la potencia de spark, posteriormente guardaremos los datos en BQ despu√©s de \n",
    "            # las transformaciones...\n",
    "            # sdf.count()\n",
    "            psdf = sdf.pandas_api().spark.cache()\n",
    "            sdf.unpersist()\n",
    "            del sdf\n",
    "            gc.collect()\n",
    "            # psdf['time'] = ps.to_datetime(psdf['time'],unit='ms')\n",
    "            psdf['estado'] = state\n",
    "            psdf['resp'] = psdf.resp.fillna('nan')\n",
    "            psdf['text'] = psdf.text.fillna('nan')\n",
    "            df_list.append(psdf)\n",
    "            i += 1\n",
    "        except AnalysisException:\n",
    "            break\n",
    "\n",
    "    psdf = ps.concat(df_list,axis=0)\n",
    "    psdf = psdf.reset_index(drop=True)\n",
    "    del df_list\n",
    "    psdf.spark.cache()\n",
    "    print(f'pyspark.pandas data frame persisted - {state}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Generamos el primer grupo de transformaciones para los datos de las reviews de Maps en PANDAS API. Queda la metadata y los archivos de Yelp.\n",
    "#     psdf['resp_time'] = ps.Series([],dtype='int64')\n",
    "#     print('serie resp_time creada')\n",
    "#     psdf['resp_text'] = ps.Series([],dtype='str')\n",
    "#     psdf.spark.cache()\n",
    "#     print('serie resp_text creada')\n",
    "#     for i in range(len(psdf)):\n",
    "#         print(i)\n",
    "#         if type(psdf.loc[i,'resp']) == dict:\n",
    "#             psdf.loc[i,'resp_time'] = psdf.loc[i,'resp']['time']\n",
    "#             psdf.loc[i,'resp_text'] = psdf.loc[i,'resp']['text']\n",
    "#         else:\n",
    "#             pass\n",
    "#     psdf.resp_time = psdf.resp_time.fillna(0).astype('int64')\n",
    "#     psdf.resp_text = psdf.resp_text.fillna('')\n",
    "#     psdf = psdf[['gmap_id','user_id','name','time','text','rating','resp_time','resp_text']]\n",
    "#     psdf.spark.cache()\n",
    "    \n",
    "#     # Aqu√≠ concatenamos todos los archivos del estado en curso a los dem√°s estados, para obtener una tabla total de estados.\n",
    "#     psdfx = ps.concat(psdf,axis=0)\n",
    "#     psdf.spark.unpersist()\n",
    "#     del psdf\n",
    "#     gc.collect()\n",
    "#     print('pyspark.pandas data frame unpersisted and deleted')\n",
    "\n",
    "# # Convertimos el dataframe de Pandas API on Spark a un dataframe de Spark\n",
    "# sdf = psdfx.to_spark()\n",
    "# del psdfx\n",
    "# gc.collect()\n",
    "\n",
    "# # Guardamos las tablas concatenadas en archivos .json en GCS.\n",
    "# sdf.write.mode('overwrite').format('csv').save(f'../data/Google Maps/clean_test/estados/all_tables')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
