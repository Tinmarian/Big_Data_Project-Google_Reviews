{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Tinmar:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[5]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Transformations to Populate our Data Warehouse</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2b2748678e0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import pandas as pd\n",
    "import ast\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable \n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = '1' \n",
    "\n",
    "# imports para trabajar con spark en local\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import IntegerType,StructField,StructType,StringType,LongType,MapType\n",
    "from pyspark.sql import functions as F\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "# imports para trabajar con GCP\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import translate_v2 as translate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Credenciales para API's de GCP\n",
    "SERVICE_ACCOUNT_FILE = '../../credentials/fiery-protocol-399500-f2566dd92ef4.json'\n",
    "creds = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE)\n",
    "translate_client = translate.Client(credentials=creds)\n",
    "\n",
    "\n",
    "# Iniciamos sesión de spark.\n",
    "SparkSession.stop(spk)\n",
    "spk = SparkSession.builder.appName(\"PySpark Transformations to Populate our Data Warehouse\") \\\n",
    "        .master(\"local[5]\") \\\n",
    "        .config(\"spark.executor.memory\",\"2g\") \\\n",
    "        .config(\"spark.driver.memory\",\"2g\") \\\n",
    "        .config(\"spark.speculation\",\"false\") \\\n",
    "        .config(\"parquet.enable.summary-metadata\", \"false\") \\\n",
    "        .config(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\") \\\n",
    "        .getOrCreate()\n",
    "ps.options.compute.ops_on_diff_frames = True\n",
    "spk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark-3.5.0\\python\\pyspark\\pandas\\utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField('user_id',StringType(),False),\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('time',LongType(),True),\n",
    "    StructField('rating',IntegerType(),True),\n",
    "    StructField('text',StringType(),True),\n",
    "    StructField('resp',MapType(StringType(),StringType()),False),\n",
    "    StructField('gmap_id',StringType(),False)\n",
    "])\n",
    "\n",
    "\n",
    "i = 1\n",
    "df_list = []\n",
    "while True:\n",
    "    try:\n",
    "        # Leemos los archivos en un SPARK Data Frame para poder acceder directamente a GCS\n",
    "        sdf = spk.read.schema(schema).json(f'../../data/Google Maps/estados/review-California/{i}.json')[['gmap_id', 'user_id', 'name', 'time', 'rating', 'text', 'resp']].cache()\n",
    "        sdf.selectExpr('cast(user_id as int) user_id').cache()\n",
    "        sdf = sdf.withColumn('tiempo_respuesta',F.col('resp').getField('time')).cache()\n",
    "        sdf = sdf.withColumn('tiempo_respuesta',F.col('tiempo_respuesta').cast(LongType())).cache()\n",
    "        sdf = sdf.withColumn('respuesta',F.col('resp').getField('text')).sort('user_id').cache()\n",
    "        sdf = sdf.withColumn('time',F.col('time').cast(LongType())).cache()\n",
    "        # PANDAS API Data Frame: Paso intermedio para trabajar con los métodos de pandas pero con la potencia de spark, posteriormente guardaremos los datos en BQ después de \n",
    "        # las transformaciones...\n",
    "        # sdf.count()\n",
    "        psdf = sdf.pandas_api()\n",
    "        sdf.unpersist()\n",
    "        del sdf\n",
    "        gc.collect()\n",
    "        psdf = psdf[['gmap_id','user_id','name','time','rating','text','tiempo_respuesta','respuesta']].reset_index(drop=True).spark.cache()\n",
    "        psdf['estado'] = 'California' # state\n",
    "        df_list.append(psdf)\n",
    "        i += 1\n",
    "    except AnalysisException:\n",
    "        break\n",
    "\n",
    "psdf = ps.concat(df_list,axis=0)\n",
    "del df_list\n",
    "gc.collect()\n",
    "\n",
    "sdf = psdf.to_spark().cache()\n",
    "# psdf.to_csv('../../data/Google Maps/clean_test/estados/california')  # 59.8s master('local[6]')\n",
    "sdf.coalesce(1).write.mode('overwrite').format('csv').save('../../data/Google Maps/clean_test/estados/california')  # 56.2s, 1m 8s master('local[6]') # 57.6s, 1m 0.4s master('local[5]') # 1m 42.1s master('local[3]') # 1m 17s master('local[9]') # 1m 52s master('local[*]') # 1m 5.1s master('local[7]')\n",
    "print(f'pyspark.pandas data frame persisted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+-------------+------+--------------------+--------------------+----------------+--------------------+\n",
      "|index|             gmap_id|             user_id|                name|         time|rating|                text|                resp|tiempo_respuesta|           respuesta|\n",
      "+-----+--------------------+--------------------+--------------------+-------------+------+--------------------+--------------------+----------------+--------------------+\n",
      "|    0|0x808fc09ce6023bc...|10000018622061881...| Magdalena Duszynska|1491184470370|     5|Great support in ...|                NULL|            NULL|                NULL|\n",
      "|    1|0x809107a3af24de1...|10000024062831966...|         Linda Thorn|1515356189482|     4|                 No!|                NULL|            NULL|                NULL|\n",
      "|    2|0x80c2d71591c3f07...|10000035378208722...|             Oso Bob|1537160465672|     2|Cockroaches and b...|                NULL|            NULL|                NULL|\n",
      "|    3|0x809ae02fcebcb76...|10000036662254181...|        Elaine Frank|1550798010453|     5|Awesome staff, ve...|                NULL|            NULL|                NULL|\n",
      "|    4|0x80dcd94f21252dd...|10000038050060612...|    Alejandro Chavez|1602477641696|     1|(Translated by Go...|{time -> 16034706...|   1603470620740|(Translated by Go...|\n",
      "|    5|0x80c2cf5207d0f89...|10000042730985697...|Juan Pablo Benavente|1531768328441|     5|                NULL|                NULL|            NULL|                NULL|\n",
      "|    6|0x80c34cbd3a51497...|10000043848837952...|         ErMac CaMre|1617511548264|     5|You could get you...|                NULL|            NULL|                NULL|\n",
      "|    7|0x80dcb059a76e7fd...|10000056335082126...|        Shady Camaro|1599782558640|     5|Grew up across th...|                NULL|            NULL|                NULL|\n",
      "|    8|0x80dd27b76fd0bf8...|10000105768380780...|  Mario Mexico Vidal|1505850299668|     4|                NULL|                NULL|            NULL|                NULL|\n",
      "|    9|0x80945c4453bdb57...|10000111996963980...|           Dan Serns|1630954898274|     5|Great people tell...|                NULL|            NULL|                NULL|\n",
      "|   10|0x80945c4453bdb57...|10000111996963980...|           Dan Serns|1630954898274|     5|Great people tell...|                NULL|            NULL|                NULL|\n",
      "|   11|0x80db7f5fb074263...|10000124190275722...|   Patrice Dreckmann|1617484607443|     5|                NULL|                NULL|            NULL|                NULL|\n",
      "|   12|0x80dcd9e79e4adbf...|10000142440775733...|   Hilah Loewenstein|1583014753239|     5|They were very ni...|                NULL|            NULL|                NULL|\n",
      "|   13|0x80857721fa01376...|10000165933712867...|      David litmonjr|1564146171133|     5|Jovance Salon is ...|                NULL|            NULL|                NULL|\n",
      "|   14|0x80857365829dab3...|10000165933712867...|      David litmonjr|1523544852655|     5|Friendly, and ver...|                NULL|            NULL|                NULL|\n",
      "|   15|0x809adaebab38dc6...|10000182619519741...|       Lori Harrison|1597952359886|     5|                NULL|                NULL|            NULL|                NULL|\n",
      "|   16|0x80dd2605c7acc11...|10000198778828170...|      Trinidad Ortiz|1568586692944|     5|                NULL|                NULL|            NULL|                NULL|\n",
      "|   17|0x80d96160792274b...|10000230862408794...|           tim mudra|1568735458924|     3|They have a lot o...|{time -> 15688473...|   1568847373567|Thank you for tak...|\n",
      "|   18|0x8094e2ba367acf9...|10000249827282723...|      Dylan Chiasson|1513211157565|     5| Very helpful staff.|                NULL|            NULL|                NULL|\n",
      "|   19|0x80dd2967346bbad...|10000266807119538...|         Evie Floram|1468027787519|     5|This is the right...|                NULL|            NULL|                NULL|\n",
      "+-----+--------------------+--------------------+--------------------+-------------+------+--------------------+--------------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = false)\n",
      " |-- gmap_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- resp: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- tiempo_respuesta: long (nullable = true)\n",
      " |-- respuesta: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_bresp(respuesta:str):\n",
    "    language = translate_client.detect_language(respuesta)\n",
    "    translation = translate_client.translate(respuesta, target_language='en')\n",
    "    if language['language'] == 'en':\n",
    "        psdf.withColumn('idioma_respuesta', 'en').cache()\n",
    "    else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf['idioma_respuesta'] = ps.Series([],dtype='str')\n",
    "respuestas.spark.cache()\n",
    "for i in resp_idx:\n",
    "    print(i)\n",
    "    respuesta = respuestas.loc[i,'resp'][30:-2]\n",
    "    translation = translate_client.translate(respuesta, target_language='en')\n",
    "    if language['language'] != 'en':\n",
    "        psdf.loc[i,'idioma_respuesta'] = language['language']\n",
    "        psdf.loc[i,'resp'] = ast.literal_eval(respuestas.loc[i,'resp'][:30] + f'{translation[\"translatedText\"]}.\"' + '}')\n",
    "    else:\n",
    "        psdf.loc[i,'idioma_respuesta'] = 'en'\n",
    "        psdf.loc[i,'resp'] = ast.literal_eval(respuestas.loc[i,'resp'])\n",
    "    if i > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'confidence': 1, 'language': 'es', 'input': 'hola cómo estás'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_client.detect_language('hola cómo estás')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translatedText': 'Hi, how are you',\n",
       " 'detectedSourceLanguage': 'es',\n",
       " 'input': 'hola cómo estás'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_client.translate('hola cómo estás', target_language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2700000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index      2700000\n",
       "gmap_id    2700000\n",
       "user_id    2700000\n",
       "name       2700000\n",
       "time       2700000\n",
       "rating     2700000\n",
       "text       1529036\n",
       "resp        245169\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>af</td>\n",
       "      <td>Afrikaans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sq</td>\n",
       "      <td>Albanian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language       name\n",
       "0       af  Afrikaans\n",
       "1       sq   Albanian"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages_psdf = ps.DataFrame(translate_client.get_languages()).sort_values('name')\n",
    "languages_psdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"time\":1629166810138,\"text\":\"现在接线员是很好的英语\"}'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf.resp.loc[66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'现在接线员是很好的英语'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf.resp.loc[66][30:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf['text'] = psdf.text.fillna('nan')\n",
    "sdf = sdf.withColumn(\"resp\", F.to_json(\"resp\"))\n",
    "respuestas = sdf.pandas_api()\n",
    "respuestas = respuestas[['resp']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\pyspark\\pandas\\frame.py:13566\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m  13565\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m> 13566\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc[:, key]\n\u001b[0;32m  13567\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\pyspark\\pandas\\indexing.py:487\u001b[0m, in \u001b[0;36mLocIndexerLike.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    480\u001b[0m cond, limit, remaining_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_rows(rows_sel)\n\u001b[0;32m    481\u001b[0m (\n\u001b[0;32m    482\u001b[0m     column_labels,\n\u001b[0;32m    483\u001b[0m     data_spark_columns,\n\u001b[0;32m    484\u001b[0m     data_fields,\n\u001b[0;32m    485\u001b[0m     returns_series,\n\u001b[0;32m    486\u001b[0m     series_name,\n\u001b[1;32m--> 487\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_select_cols(cols_sel)\n\u001b[0;32m    489\u001b[0m \u001b[39mif\u001b[39;00m cond \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m limit \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m returns_series:\n",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\pyspark\\pandas\\indexing.py:334\u001b[0m, in \u001b[0;36mLocIndexerLike._select_cols\u001b[1;34m(self, cols_sel, missing_keys)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_select_cols_else(cols_sel, missing_keys)\n",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\pyspark\\pandas\\indexing.py:1392\u001b[0m, in \u001b[0;36mLocIndexer._select_cols_else\u001b[1;34m(self, cols_sel, missing_keys)\u001b[0m\n\u001b[0;32m   1391\u001b[0m     cols_sel \u001b[39m=\u001b[39m (cols_sel,)\n\u001b[1;32m-> 1392\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_from_multiindex_column(cols_sel, missing_keys)\n",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\pyspark\\pandas\\indexing.py:1210\u001b[0m, in \u001b[0;36mLocIndexer._get_from_multiindex_column\u001b[1;34m(self, key, missing_keys, labels, recursed)\u001b[0m\n\u001b[0;32m   1209\u001b[0m \u001b[39mif\u001b[39;00m missing_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1210\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(k)\n\u001b[0;32m   1211\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tinma\\OneDrive\\Escritorio\\HENRY\\Proyecto_Grupal_HENRY\\spark\\local_testing\\pyspark_testing.ipynb Celda 13\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tinma/OneDrive/Escritorio/HENRY/Proyecto_Grupal_HENRY/spark/local_testing/pyspark_testing.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m respuestas\u001b[39m.\u001b[39mspark\u001b[39m.\u001b[39mcache()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tinma/OneDrive/Escritorio/HENRY/Proyecto_Grupal_HENRY/spark/local_testing/pyspark_testing.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# respuestas = respuestas.astype({'resp':str}).spark.cache()\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tinma/OneDrive/Escritorio/HENRY/Proyecto_Grupal_HENRY/spark/local_testing/pyspark_testing.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m respuestas[\u001b[39m'\u001b[39m\u001b[39mresp\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m respuestas\u001b[39m.\u001b[39;49mtext\u001b[39m.\u001b[39mfillna(\u001b[39m'\u001b[39m\u001b[39mnan\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tinma/OneDrive/Escritorio/HENRY/Proyecto_Grupal_HENRY/spark/local_testing/pyspark_testing.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m respuestas \u001b[39m=\u001b[39m respuestas\u001b[39m.\u001b[39mresp \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnan\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tinma/OneDrive/Escritorio/HENRY/Proyecto_Grupal_HENRY/spark/local_testing/pyspark_testing.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m resp_idx \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(respuestas\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mtolist())\n",
      "File \u001b[1;32mC:\\spark-3.5.0\\python\\pyspark\\pandas\\frame.py:13568\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m  13566\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc[:, key]\n\u001b[0;32m  13567\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m> 13568\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m  13569\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, key)\n\u001b[0;32m  13570\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "psdf['text'] = psdf.text.fillna('nan')\n",
    "sdf = sdf.withColumn(\"resp\", F.to_json(\"resp\"))\n",
    "respuestas = sdf.pandas_api()\n",
    "respuestas = respuestas[['resp']]\n",
    "respuestas.spark.cache()\n",
    "respuestas['resp'] = respuestas.text.fillna('nan')\n",
    "respuestas = respuestas.resp != 'nan'\n",
    "resp_idx = sorted(respuestas.index.tolist())\n",
    "print(respuestas.head(2))\n",
    "resp_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"time\":1629166810138,\"text\":\"The operator now speaks very good English.\"}'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf.resp.loc[66][:30] + f'{translation[\"translatedText\"]}.\"' + '}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': 1629166810138, 'text': 'The operator now speaks very good English.'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(psdf.resp.loc[66][:30] + f'{translation[\"translatedText\"]}.\"' + '}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"time\":1631072651706,\"text\":\"Thanks so much for your business and for taking the time to post a great review. Glad to hear our crew was so helpful and friendly and that we were able to get the family outfitted quickly and efficiently. Look forward to hearing from you in the future.\"}'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuestas.loc[30,'resp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(respuestas)):\n",
    "    respuestas['resp'].apply(lambda x: ast.literal_eval(x))\n",
    "    respuestas.loc[i,'resp_time'] = respuestas.loc[i,'resp']['time']\n",
    "    respuestas.loc[i,'resp_text'] = respuestas.loc[i,'resp']['text']\n",
    "respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'fiery-protocol-399500'\n",
    "STATES = ['California'] #,'Texas'] # ,'New_York','Colorado','Georgia']\n",
    "schema = StructType([\n",
    "    StructField('user_id',StringType(),False),\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('time',LongType(),True),\n",
    "    StructField('rating',IntegerType(),True),\n",
    "    StructField('text',StringType(),True),\n",
    "    StructField('resp',StringType(),False),\n",
    "    StructField('gmap_id',StringType(),False)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "psdfx = ps.DataFrame(columns=['gmap_id','user_id','name','time','text','rating','resp_time','resp_text'])\n",
    "for state in STATES:\n",
    "    i = 1\n",
    "    df_list = []\n",
    "    while True:\n",
    "        try:\n",
    "            # Leemos los archivos en un SPARK Data Frame para poder acceder directamente a GCS\n",
    "            sdf = spk.read.schema(schema).json(f'../data/Google Maps/estados/review-{state}/{i}.json')[['user_id','name','time','rating','text','resp','gmap_id']].cache()\n",
    "            sdf.selectExpr('cast(user_id as int) user_id')\n",
    "            sdf.selectExpr('cast(null as map<string,string>) resp')\n",
    "            # PANDAS API Data Frame: Paso intermedio para trabajar con los métodos de pandas pero con la potencia de spark, posteriormente guardaremos los datos en BQ después de \n",
    "            # las transformaciones...\n",
    "            # sdf.count()\n",
    "            psdf = sdf.pandas_api().spark.cache()\n",
    "            sdf.unpersist()\n",
    "            del sdf\n",
    "            gc.collect()\n",
    "            # psdf['time'] = ps.to_datetime(psdf['time'],unit='ms')\n",
    "            psdf['estado'] = state\n",
    "            psdf['resp'] = psdf.resp.fillna('nan')\n",
    "            psdf['text'] = psdf.text.fillna('nan')\n",
    "            df_list.append(psdf)\n",
    "            i += 1\n",
    "        except AnalysisException:\n",
    "            break\n",
    "\n",
    "    psdf = ps.concat(df_list,axis=0)\n",
    "    psdf = psdf.reset_index(drop=True)\n",
    "    del df_list\n",
    "    psdf.spark.cache()\n",
    "    print(f'pyspark.pandas data frame persisted - {state}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Generamos el primer grupo de transformaciones para los datos de las reviews de Maps en PANDAS API. Queda la metadata y los archivos de Yelp.\n",
    "#     psdf['resp_time'] = ps.Series([],dtype='int64')\n",
    "#     print('serie resp_time creada')\n",
    "#     psdf['resp_text'] = ps.Series([],dtype='str')\n",
    "#     psdf.spark.cache()\n",
    "#     print('serie resp_text creada')\n",
    "#     for i in range(len(psdf)):\n",
    "#         print(i)\n",
    "#         if type(psdf.loc[i,'resp']) == dict:\n",
    "#             psdf.loc[i,'resp_time'] = psdf.loc[i,'resp']['time']\n",
    "#             psdf.loc[i,'resp_text'] = psdf.loc[i,'resp']['text']\n",
    "#         else:\n",
    "#             pass\n",
    "#     psdf.resp_time = psdf.resp_time.fillna(0).astype('int64')\n",
    "#     psdf.resp_text = psdf.resp_text.fillna('')\n",
    "#     psdf = psdf[['gmap_id','user_id','name','time','text','rating','resp_time','resp_text']]\n",
    "#     psdf.spark.cache()\n",
    "    \n",
    "#     # Aquí concatenamos todos los archivos del estado en curso a los demás estados, para obtener una tabla total de estados.\n",
    "#     psdfx = ps.concat(psdf,axis=0)\n",
    "#     psdf.spark.unpersist()\n",
    "#     del psdf\n",
    "#     gc.collect()\n",
    "#     print('pyspark.pandas data frame unpersisted and deleted')\n",
    "\n",
    "# # Convertimos el dataframe de Pandas API on Spark a un dataframe de Spark\n",
    "# sdf = psdfx.to_spark()\n",
    "# del psdfx\n",
    "# gc.collect()\n",
    "\n",
    "# # Guardamos las tablas concatenadas en archivos .json en GCS.\n",
    "# sdf.write.mode('overwrite').format('csv').save(f'../data/Google Maps/clean_test/estados/all_tables')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
